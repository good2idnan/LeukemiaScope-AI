{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "0KhzZ3FBt9k4"
      },
      "source": [
        "# MedGemma ALL Leukemia \u2013 Dataset Prep (2 Kaggle datasets)\n",
        "This notebook:\n",
        "- Downloads 2 Kaggle datasets\n",
        "- Loads maximum images (ALL=1, HEM=0)\n",
        "- Checks image types, sizes, and simple background stats\n",
        "- Deduplicates by file hash\n",
        "- Creates stratified train/val/test splits\n"
      ],
      "id": "0KhzZ3FBt9k4"
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "31H6KcP5t9k5",
        "outputId": "6089486f-242e-4226-8f37-1e12af60a9bd"
      },
      "source": [
        "!pip install -q transformers>=4.45.0\n",
        "!pip install -q peft>=0.13.0\n",
        "!pip install -q accelerate>=0.34.0\n",
        "!pip install -q bitsandbytes>=0.44.0\n",
        "!pip install -q datasets>=3.0.0\n",
        "!pip install -q pillow\n",
        "!pip install -q tqdm\n",
        "!pip install -q huggingface_hub\n",
        "\n",
        "print(\"\u2705 All packages installed!\")"
      ],
      "id": "31H6KcP5t9k5",
      "execution_count": 33,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\u2705 All packages installed!\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "y9oTOGqRt9k6"
      },
      "source": [
        "## \ud83d\udd10 Hugging Face login\n",
        "**Important:** Do NOT hardcode your HF token in notebooks. Use an environment variable instead.\n",
        "If you already pasted a token in a notebook/chat, revoke it on Hugging Face and create a new one."
      ],
      "id": "y9oTOGqRt9k6"
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "z9JTvuMbt9k6",
        "outputId": "b8f1dfb2-2fa6-47c7-fcf7-be9b51ac458d"
      },
      "source": [
        "from huggingface_hub import login\n",
        "import os, getpass\n",
        "\n",
        "# Option A: if you already set HF_TOKEN in Colab secrets / env:\n",
        "token = os.environ.get(\"HF_TOKEN\", \"\")\n",
        "\n",
        "# Option B: prompt securely (recommended)\n",
        "if not token:\n",
        "    token = getpass.getpass(\"Paste your HuggingFace token (input hidden): \")\n",
        "\n",
        "login(token=token)\n",
        "print(\"\u2705 Logged in!\")\n"
      ],
      "id": "z9JTvuMbt9k6",
      "execution_count": 34,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Paste your HuggingFace token (input hidden): \u00b7\u00b7\u00b7\u00b7\u00b7\u00b7\u00b7\u00b7\u00b7\u00b7\n",
            "\u2705 Logged in!\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "7_v0MF1Ot9k6"
      },
      "source": [
        "## \ud83d\udce6 Download Kaggle datasets (kagglehub)"
      ],
      "id": "7_v0MF1Ot9k6"
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "7bRPQPdut9k6",
        "outputId": "89dc8cfc-cc2e-4cb5-d825-6c124716504d"
      },
      "source": [
        "import kagglehub, os\n",
        "\n",
        "# Dataset A: C-NMC Leukemia (ALL vs HEM)\n",
        "path_a = kagglehub.dataset_download(\"andrewmvd/leukemia-classification\")\n",
        "print(\"\u2705 Dataset A downloaded to:\", path_a)\n",
        "print(\"\ud83d\udcc2 Top-level entries:\", os.listdir(path_a))\n",
        "\n",
        "# Dataset B: Leukemia Image Dataset (ALL vs HEM)\n",
        "path_b = kagglehub.dataset_download(\"rakibhasan3948/leukemia-image-dataset\")\n",
        "print(\"\\n\u2705 Dataset B downloaded to:\", path_b)\n",
        "print(\"\ud83d\udcc2 Top-level entries:\", os.listdir(path_b))\n"
      ],
      "id": "7bRPQPdut9k6",
      "execution_count": 35,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Using Colab cache for faster access to the 'leukemia-classification' dataset.\n",
            "\u2705 Dataset A downloaded to: /kaggle/input/leukemia-classification\n",
            "\ud83d\udcc2 Top-level entries: ['C-NMC_Leukemia']\n",
            "Using Colab cache for faster access to the 'leukemia-image-dataset' dataset.\n",
            "\n",
            "\u2705 Dataset B downloaded to: /kaggle/input/leukemia-image-dataset\n",
            "\ud83d\udcc2 Top-level entries: ['Leukemia-Image-Dataset']\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "BWjSsLBct9k6"
      },
      "source": [
        "## \ud83e\udded Load maximum images + label mapping\n",
        "- **ALL / all** \u2192 `1` (Leukemia)\n",
        "- **HEM / hem** \u2192 `0` (Normal)\n",
        "\n",
        "We also **deduplicate** by file hash."
      ],
      "id": "BWjSsLBct9k6"
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "kkeGX5Eqt9k6",
        "outputId": "265a436d-d325-4aa4-a156-c335ce4ceb42"
      },
      "source": [
        "import os\n",
        "from pathlib import Path\n",
        "from collections import Counter\n",
        "\n",
        "IMG_EXTS = {\".jpg\", \".jpeg\", \".png\", \".bmp\", \".tif\", \".tiff\", \".webp\"}\n",
        "\n",
        "def collect_images(root: Path):\n",
        "    root = Path(root)\n",
        "    out = []\n",
        "    for dp, _, fns in os.walk(root):\n",
        "        dp = Path(dp)\n",
        "        for fn in fns:\n",
        "            if Path(fn).suffix.lower() in IMG_EXTS:\n",
        "                out.append(dp / fn)\n",
        "    return out\n",
        "\n",
        "def find_dir_anywhere(root: Path, target: str):\n",
        "    \"\"\"Find a folder named `target` anywhere under root, case-insensitive.\"\"\"\n",
        "    target = target.lower()\n",
        "    for p in root.rglob(\"*\"):\n",
        "        if p.is_dir() and p.name.lower() == target:\n",
        "            return p\n",
        "    return None\n",
        "\n",
        "def clean_pairs(images, labels, name=\"dataset\"):\n",
        "    \"\"\"\n",
        "    Safety cleaner:\n",
        "      - trims to min length if mismatch\n",
        "      - drops missing files\n",
        "      - de-duplicates by full path (keeps first label; warns on conflicts)\n",
        "    \"\"\"\n",
        "    images = list(images)\n",
        "    labels = list(labels)\n",
        "\n",
        "    if len(images) != len(labels):\n",
        "        print(f\"\u26a0\ufe0f {name} mismatch: images={len(images)}, labels={len(labels)} -> trimming to min\")\n",
        "        n = min(len(images), len(labels))\n",
        "        images, labels = images[:n], labels[:n]\n",
        "\n",
        "    # drop missing\n",
        "    kept_imgs, kept_lbls = [], []\n",
        "    missing = 0\n",
        "    for p, y in zip(images, labels):\n",
        "        if Path(p).exists():\n",
        "            kept_imgs.append(str(p))\n",
        "            kept_lbls.append(int(y))\n",
        "        else:\n",
        "            missing += 1\n",
        "    if missing:\n",
        "        print(f\"\u26a0\ufe0f {name}: removed missing files: {missing}\")\n",
        "\n",
        "    # dedupe by path\n",
        "    seen = {}\n",
        "    dedup_imgs, dedup_lbls = [], []\n",
        "    conflicts = 0\n",
        "    for p, y in zip(kept_imgs, kept_lbls):\n",
        "        if p in seen:\n",
        "            if seen[p] != y:\n",
        "                conflicts += 1\n",
        "            continue\n",
        "        seen[p] = y\n",
        "        dedup_imgs.append(p)\n",
        "        dedup_lbls.append(y)\n",
        "\n",
        "    if conflicts:\n",
        "        print(f\"\u26a0\ufe0f {name}: duplicate path label conflicts: {conflicts} (kept first label)\")\n",
        "\n",
        "    print(f\"\u2705 {name}: final={len(dedup_imgs)}  Normal={sum(1 for v in dedup_lbls if v==0)}  Leukemia={sum(1 for v in dedup_lbls if v==1)}\")\n",
        "    return dedup_imgs, dedup_lbls\n",
        "\n",
        "def safe_combine(datasets):\n",
        "    \"\"\"datasets = [(name, images, labels), ...] -> combined (images, labels)\"\"\"\n",
        "    all_imgs, all_lbls = [], []\n",
        "    for name, imgs, lbls in datasets:\n",
        "        imgs2, lbls2 = clean_pairs(imgs, lbls, name=name)\n",
        "        all_imgs.extend(imgs2)\n",
        "        all_lbls.extend(lbls2)\n",
        "\n",
        "    assert len(all_imgs) == len(all_lbls), \"\u274c Internal error: combined lengths still mismatch.\"\n",
        "    print(\"\\n\u2705 Combined datasets (MAX images, cleaned)\")\n",
        "    print(\"Total images:\", len(all_imgs))\n",
        "    print(\"Normal(0):\", sum(l==0 for l in all_lbls))\n",
        "    print(\"Leukemia(1):\", sum(l==1 for l in all_lbls))\n",
        "    return all_imgs, all_lbls\n",
        "\n",
        "# -------------------------\n",
        "# Dataset A: C-NMC Leukemia\n",
        "# training_data/fold_0,1,2/{all,hem}\n",
        "# all -> 1 (Leukemia), hem -> 0 (Normal)\n",
        "# -------------------------\n",
        "def load_cnm_all_folds(path_a_root: str, folds=(\"fold_0\",\"fold_1\",\"fold_2\")):\n",
        "    base = Path(path_a_root) / \"C-NMC_Leukemia\" / \"training_data\"\n",
        "    images, labels = [], []\n",
        "\n",
        "    if not base.exists():\n",
        "        print(\"\u26a0\ufe0f C-NMC training_data not found at:\", base)\n",
        "        print(\"Top-level:\", [p.name for p in Path(path_a_root).iterdir()])\n",
        "        return [], []\n",
        "\n",
        "    for fold in folds:\n",
        "        fold_dir = base / fold\n",
        "        if not fold_dir.exists():\n",
        "            print(f\"\u26a0\ufe0f Missing fold folder: {fold_dir}\")\n",
        "            continue\n",
        "\n",
        "        for cls, y in [(\"all\", 1), (\"hem\", 0)]:\n",
        "            cls_dir = fold_dir / cls\n",
        "            if cls_dir.exists():\n",
        "                imgs = collect_images(cls_dir)\n",
        "                images.extend([str(p) for p in imgs])\n",
        "                labels.extend([y] * len(imgs))\n",
        "            else:\n",
        "                print(f\"\u26a0\ufe0f Missing class folder: {cls_dir}\")\n",
        "\n",
        "    print(\"\\n\u2705 Loaded Dataset A (C-NMC)\")\n",
        "    print(\"  Total:\", len(images),\n",
        "          \" Normal:\", sum(l==0 for l in labels),\n",
        "          \" Leukemia:\", sum(l==1 for l in labels))\n",
        "    return images, labels\n",
        "\n",
        "# -------------------------\n",
        "# Dataset B: Leukemia Image Dataset (Rakib)\n",
        "# folders: ALL (cancer), HEM (normal) but may be nested\n",
        "# ALL -> 1, HEM -> 0\n",
        "# -------------------------\n",
        "def load_rakib_all_hem(path_b_root: str):\n",
        "    root = Path(path_b_root)\n",
        "\n",
        "    all_dir = find_dir_anywhere(root, \"ALL\")\n",
        "    hem_dir = find_dir_anywhere(root, \"HEM\")\n",
        "\n",
        "    print(\"\\n\ud83d\udd0e Dataset B folder detection\")\n",
        "    print(\"  ALL dir:\", all_dir)\n",
        "    print(\"  HEM dir:\", hem_dir)\n",
        "\n",
        "    images, labels = [], []\n",
        "\n",
        "    if all_dir is not None:\n",
        "        imgs = collect_images(all_dir)\n",
        "        images.extend([str(p) for p in imgs])\n",
        "        labels.extend([1] * len(imgs))\n",
        "\n",
        "    if hem_dir is not None:\n",
        "        imgs = collect_images(hem_dir)\n",
        "        images.extend([str(p) for p in imgs])\n",
        "        labels.extend([0] * len(imgs))\n",
        "\n",
        "    print(\"\\n\u2705 Loaded Dataset B (Rakib Hasan)\")\n",
        "    print(\"  Total:\", len(images),\n",
        "          \" Normal:\", sum(l==0 for l in labels),\n",
        "          \" Leukemia:\", sum(l==1 for l in labels))\n",
        "\n",
        "    if len(images) == 0:\n",
        "        print(\"\u26a0\ufe0f Dataset B returned 0 images.\")\n",
        "        print(\"\ud83d\udc49 Top-level entries:\", [p.name for p in root.iterdir()])\n",
        "\n",
        "    return images, labels\n",
        "\n",
        "# -------------------------\n",
        "# LOAD BOTH DATASETS (clean + safe)\n",
        "# -------------------------\n",
        "a_images, a_labels = load_cnm_all_folds(path_a)\n",
        "b_images, b_labels = load_rakib_all_hem(path_b)\n",
        "\n",
        "all_images, all_labels = safe_combine([\n",
        "    (\"DatasetA_CNM\", a_images, a_labels),\n",
        "    (\"DatasetB_Rakib\", b_images, b_labels),\n",
        "])\n"
      ],
      "id": "kkeGX5Eqt9k6",
      "execution_count": 36,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "\u2705 Loaded Dataset A (C-NMC)\n",
            "  Total: 10661  Normal: 3389  Leukemia: 7272\n",
            "\n",
            "\ud83d\udd0e Dataset B folder detection\n",
            "  ALL dir: /kaggle/input/leukemia-image-dataset/Leukemia-Image-Dataset/ALL\n",
            "  HEM dir: /kaggle/input/leukemia-image-dataset/Leukemia-Image-Dataset/HEM\n",
            "\n",
            "\u2705 Loaded Dataset B (Rakib Hasan)\n",
            "  Total: 6778  Normal: 3389  Leukemia: 3389\n",
            "\u2705 DatasetA_CNM: final=10661  Normal=3389  Leukemia=7272\n",
            "\u2705 DatasetB_Rakib: final=6778  Normal=3389  Leukemia=3389\n",
            "\n",
            "\u2705 Combined datasets (MAX images, cleaned)\n",
            "Total images: 17439\n",
            "Normal(0): 6778\n",
            "Leukemia(1): 10661\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "pd7MtAk4t9k6"
      },
      "source": [
        "## \ud83d\uddbc\ufe0f Check image type, size, and simple background stats\n",
        "We sample images for speed and report:\n",
        "- format distribution\n",
        "- width/height distribution\n",
        "- mean border brightness (rough background indicator)\n"
      ],
      "id": "pd7MtAk4t9k6"
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "pQghJMbft9k7",
        "outputId": "5b761a3f-4e1a-4391-cc95-cb9f2cb2f7fb"
      },
      "source": [
        "from PIL import Image\n",
        "import numpy as np\n",
        "import random\n",
        "from pathlib import Path\n",
        "from collections import Counter\n",
        "from tqdm import tqdm\n",
        "\n",
        "# Avoid PIL decompression bomb warnings for large images\n",
        "Image.MAX_IMAGE_PIXELS = None\n",
        "\n",
        "def border_mean_intensity(img: Image.Image, border=4):\n",
        "    \"\"\"\n",
        "    Mean grayscale intensity of border pixels.\n",
        "    0=black, 255=white.\n",
        "    \"\"\"\n",
        "    # Convert to 8-bit grayscale safely (handles TIFF / 16-bit cases better)\n",
        "    arr = np.array(img.convert(\"L\"), dtype=np.uint8)\n",
        "    h, w = arr.shape\n",
        "    if h == 0 or w == 0:\n",
        "        return float(\"nan\")\n",
        "\n",
        "    b = min(border, max(1, h // 8), max(1, w // 8))  # robust border width\n",
        "\n",
        "    # Border without double-counting corners too much\n",
        "    top = arr[:b, :]\n",
        "    bottom = arr[-b:, :]\n",
        "    left = arr[b:-b, :b] if h > 2*b else arr[:, :b]\n",
        "    right = arr[b:-b, -b:] if h > 2*b else arr[:, -b:]\n",
        "\n",
        "    border_pixels = np.concatenate([top.ravel(), bottom.ravel(), left.ravel(), right.ravel()])\n",
        "    return float(border_pixels.mean())\n",
        "\n",
        "def inspect_images(paths, sample_n=800, seed=42, border=4):\n",
        "    \"\"\"\n",
        "    Inspects image format distribution, size distribution, and border intensity.\n",
        "    \"\"\"\n",
        "    paths = list(paths)\n",
        "    if len(paths) == 0:\n",
        "        print(\"\u26a0\ufe0f No image paths provided.\")\n",
        "        return\n",
        "\n",
        "    random.seed(seed)\n",
        "    sample = paths if len(paths) <= sample_n else random.sample(paths, sample_n)\n",
        "\n",
        "    formats, sizes, border_means = [], [], []\n",
        "    failures = 0\n",
        "\n",
        "    for p in tqdm(sample, desc=f\"Inspecting {len(sample)} images\"):\n",
        "        try:\n",
        "            p = str(p)\n",
        "            with Image.open(p) as im:\n",
        "                fmt = (im.format or Path(p).suffix.replace(\".\", \"\") or \"unknown\").lower()\n",
        "                formats.append(fmt)\n",
        "                sizes.append(im.size)  # (w,h)\n",
        "                border_means.append(border_mean_intensity(im, border=border))\n",
        "        except Exception as e:\n",
        "            failures += 1\n",
        "\n",
        "    if len(sizes) == 0:\n",
        "        print(f\"\u274c All failed to open. Failures: {failures} / {len(sample)}\")\n",
        "        return\n",
        "\n",
        "    w_list = [s[0] for s in sizes]\n",
        "    h_list = [s[1] for s in sizes]\n",
        "    b_list = [x for x in border_means if np.isfinite(x)]\n",
        "\n",
        "    print(\"\\n\u2705 Inspected:\", len(sample), \" Failures:\", failures)\n",
        "    print(\"Formats:\", Counter(formats).most_common(10))\n",
        "    print(\"Width  min/median/max:\", min(w_list), int(np.median(w_list)), max(w_list))\n",
        "    print(\"Height min/median/max:\", min(h_list), int(np.median(h_list)), max(h_list))\n",
        "\n",
        "    if len(b_list) > 0:\n",
        "        print(\"Border mean intensity (0=black,255=white) min/median/max:\",\n",
        "              round(float(np.min(b_list)), 2),\n",
        "              round(float(np.median(b_list)), 2),\n",
        "              round(float(np.max(b_list)), 2))\n",
        "    else:\n",
        "        print(\"Border mean intensity: N/A (no valid values)\")\n",
        "\n",
        "# ---- Run on your list of image paths\n",
        "inspect_images(all_images, sample_n=800, seed=42, border=4)\n"
      ],
      "id": "pQghJMbft9k7",
      "execution_count": 37,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Inspecting 800 images: 100%|\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588| 800/800 [00:01<00:00, 574.71it/s]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "\u2705 Inspected: 800  Failures: 0\n",
            "Formats: [('bmp', 800)]\n",
            "Width  min/median/max: 450 450 450\n",
            "Height min/median/max: 450 450 450\n",
            "Border mean intensity (0=black,255=white) min/median/max: 0.0 0.0 2.11\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "dYsCS6mBt9k7"
      },
      "source": [
        "## \u2702\ufe0f Stratified Train/Val/Test split\n",
        "Default:\n",
        "- Train 80%\n",
        "- Val 10%\n",
        "- Test 10%"
      ],
      "id": "dYsCS6mBt9k7"
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "se1nNm_pt9k7",
        "outputId": "0b5bdd2f-059e-4dee-b68e-a1dc9b9ca5d2"
      },
      "source": [
        "from sklearn.model_selection import train_test_split\n",
        "from collections import Counter\n",
        "\n",
        "def split_train_val_test(image_paths, labels, seed=42, val_size=0.10, test_size=0.10):\n",
        "    assert len(image_paths) == len(labels), \"\u274c image_paths and labels length mismatch!\"\n",
        "\n",
        "    # First split: Train vs (Val+Test)\n",
        "    X_train, X_tmp, y_train, y_tmp = train_test_split(\n",
        "        image_paths,\n",
        "        labels,\n",
        "        test_size=(val_size + test_size),\n",
        "        random_state=seed,\n",
        "        stratify=labels\n",
        "    )\n",
        "\n",
        "    # Second split: Val vs Test (split X_tmp into two parts)\n",
        "    relative_test_size = test_size / (val_size + test_size)\n",
        "    X_val, X_test, y_val, y_test = train_test_split(\n",
        "        X_tmp,\n",
        "        y_tmp,\n",
        "        test_size=relative_test_size,\n",
        "        random_state=seed,\n",
        "        stratify=y_tmp\n",
        "    )\n",
        "\n",
        "    def summarize(name, y):\n",
        "        c = Counter(map(int, y))\n",
        "        print(f\"{name}: {len(y):6d}  Normal(0)={c.get(0,0):6d}  Leukemia(1)={c.get(1,0):6d}\")\n",
        "\n",
        "    print(\"\\n\u2705 Split summary\")\n",
        "    summarize(\"Train\", y_train)\n",
        "    summarize(\"Val  \", y_val)\n",
        "    summarize(\"Test \", y_test)\n",
        "\n",
        "    # Overlap check\n",
        "    s_train, s_val, s_test = set(X_train), set(X_val), set(X_test)\n",
        "    print(\"\\n\ud83d\udd0d Overlap check\")\n",
        "    print(\"Train \u2229 Val :\", len(s_train & s_val))\n",
        "    print(\"Train \u2229 Test:\", len(s_train & s_test))\n",
        "    print(\"Val   \u2229 Test:\", len(s_val & s_test))\n",
        "\n",
        "    return X_train, y_train, X_val, y_val, X_test, y_test\n",
        "\n",
        "\n",
        "# \u2705 Use your combined lists here:\n",
        "train_images, train_labels, val_images, val_labels, test_images, test_labels = split_train_val_test(\n",
        "    all_images, all_labels, seed=42, val_size=0.10, test_size=0.10\n",
        ")\n"
      ],
      "id": "se1nNm_pt9k7",
      "execution_count": 38,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "\u2705 Split summary\n",
            "Train:  13951  Normal(0)=  5422  Leukemia(1)=  8529\n",
            "Val  :   1744  Normal(0)=   678  Leukemia(1)=  1066\n",
            "Test :   1744  Normal(0)=   678  Leukemia(1)=  1066\n",
            "\n",
            "\ud83d\udd0d Overlap check\n",
            "Train \u2229 Val : 0\n",
            "Train \u2229 Test: 0\n",
            "Val   \u2229 Test: 0\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "SXCTxKfat9k7"
      },
      "source": [
        "## \ud83d\udcbe Load model\n",
        "\n",
        "These CSVs feed directly into your fine-tuning notebook."
      ],
      "id": "SXCTxKfat9k7"
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 138,
          "referenced_widgets": [
            "07b6e2f99c6a431197e8416dee6e6d65",
            "7f452f8bcd7d43b688abd21a0beb5a82",
            "0a485518f951412197981c7526628eaf",
            "11c31810a34044668bc64b1d4617893e",
            "41afc65b9994463097b880b4225706c4",
            "d88b951fcb5040df9cb675ebea74b9dc",
            "07966f98c448424787f23e02903f7968",
            "853c528ebb6d414381848f0284a540fa",
            "eaa0bfbe19cd4e7d8dff88e533212a33",
            "5712ba10eb8049ec8952641181fa4b3e",
            "48972cb413ef41c594d87fd1641d38e8"
          ]
        },
        "id": "t9L7P4t2t9k7",
        "outputId": "f037f7d2-da0c-40fe-e374-d793b9e34eb4"
      },
      "source": [
        "import torch\n",
        "from transformers import AutoProcessor, AutoModelForImageTextToText\n",
        "\n",
        "MODEL_ID = \"google/medgemma-1.5-4b-it\"\n",
        "\n",
        "# GPU info\n",
        "assert torch.cuda.is_available(), \"CUDA not available\"\n",
        "print(f\"GPU: {torch.cuda.get_device_name(0)}\")\n",
        "print(f\"VRAM: {torch.cuda.get_device_properties(0).total_memory / 1e9:.1f} GB\")\n",
        "\n",
        "# Enable TF32 for speed (safe)\n",
        "torch.backends.cuda.matmul.allow_tf32 = True\n",
        "torch.backends.cudnn.allow_tf32 = True\n",
        "\n",
        "# Processor\n",
        "processor = AutoProcessor.from_pretrained(MODEL_ID)\n",
        "\n",
        "# Model (NO flash attention)\n",
        "model = AutoModelForImageTextToText.from_pretrained(\n",
        "    MODEL_ID,\n",
        "    torch_dtype=torch.bfloat16,\n",
        "    device_map={\"\": 0},   # force full GPU\n",
        ")\n",
        "\n",
        "model.train()\n",
        "\n",
        "print(\"\\n\u2705 Model loaded successfully\")\n",
        "print(f\"Parameters: {model.num_parameters() / 1e9:.2f} B\")\n"
      ],
      "id": "t9L7P4t2t9k7",
      "execution_count": 39,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "GPU: NVIDIA A100-SXM4-80GB\n",
            "VRAM: 85.2 GB\n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "Loading weights:   0%|          | 0/883 [00:00<?, ?it/s]"
            ],
            "application/vnd.jupyter.widget-view+json": {
              "version_major": 2,
              "version_minor": 0,
              "model_id": "07b6e2f99c6a431197e8416dee6e6d65"
            }
          },
          "metadata": {}
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "\u2705 Model loaded successfully\n",
            "Parameters: 4.30 B\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from peft import LoraConfig, get_peft_model, TaskType\n",
        "\n",
        "lora_config = LoraConfig(\n",
        "    r=32,                 # stronger than 16\n",
        "    lora_alpha=64,        # usually 2*r\n",
        "    lora_dropout=0.05,\n",
        "    target_modules=[\"q_proj\", \"k_proj\", \"v_proj\", \"o_proj\"],\n",
        "    bias=\"none\",\n",
        "    task_type=TaskType.CAUSAL_LM,\n",
        ")\n",
        "\n",
        "model = get_peft_model(model, lora_config)\n",
        "model.print_trainable_parameters()\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "2SOhyW3jz8Gr",
        "outputId": "f94bf117-0b02-4eb1-ee4c-9bd571ca2433"
      },
      "id": "2SOhyW3jz8Gr",
      "execution_count": 40,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "trainable params: 23,797,760 || all params: 4,323,877,232 || trainable%: 0.5504\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from torch.utils.data import Dataset\n",
        "from PIL import Image\n",
        "import torch\n",
        "\n",
        "class LeukemiaAnswerOnlyDataset(Dataset):\n",
        "    def __init__(self, image_paths, labels, processor, max_length=256):\n",
        "        self.image_paths = list(image_paths)\n",
        "        self.labels = [int(x) for x in labels]\n",
        "        self.processor = processor\n",
        "        self.max_length = max_length\n",
        "        self.answers = {0: \"Normal\", 1: \"Leukemia\"}\n",
        "\n",
        "        self.user_text = (\n",
        "            \"Classify this blood cell microscopy image.\\n\"\n",
        "            \"Answer with exactly ONE word: Normal or Leukemia.\\n\"\n",
        "            \"Answer:\"\n",
        "        )\n",
        "\n",
        "    def __len__(self):\n",
        "        return len(self.image_paths)\n",
        "\n",
        "    def __getitem__(self, idx):\n",
        "        image = Image.open(self.image_paths[idx]).convert(\"RGB\")\n",
        "        y = self.labels[idx]\n",
        "        answer = self.answers[y]\n",
        "\n",
        "        # full chat with answer\n",
        "        msgs_full = [\n",
        "            {\"role\": \"user\", \"content\": [{\"type\":\"image\"}, {\"type\":\"text\", \"text\": self.user_text}]},\n",
        "            {\"role\": \"assistant\", \"content\": [{\"type\":\"text\", \"text\": answer}]},\n",
        "        ]\n",
        "        text_full = self.processor.apply_chat_template(msgs_full, add_generation_prompt=False)\n",
        "\n",
        "        # prompt-only chat (empty answer)\n",
        "        msgs_prompt = [\n",
        "            {\"role\": \"user\", \"content\": [{\"type\":\"image\"}, {\"type\":\"text\", \"text\": self.user_text}]},\n",
        "            {\"role\": \"assistant\", \"content\": [{\"type\":\"text\", \"text\": \"\"}]},\n",
        "        ]\n",
        "        text_prompt = self.processor.apply_chat_template(msgs_prompt, add_generation_prompt=False)\n",
        "\n",
        "        full = self.processor(\n",
        "            images=image, text=text_full,\n",
        "            return_tensors=\"pt\", padding=\"max_length\", truncation=True, max_length=self.max_length\n",
        "        )\n",
        "        prompt = self.processor(\n",
        "            images=image, text=text_prompt,\n",
        "            return_tensors=\"pt\", padding=\"max_length\", truncation=True, max_length=self.max_length\n",
        "        )\n",
        "\n",
        "        input_ids = full[\"input_ids\"].squeeze(0)\n",
        "        attention_mask = full[\"attention_mask\"].squeeze(0)\n",
        "        pixel_values = full[\"pixel_values\"].squeeze(0)\n",
        "\n",
        "        # Answer-only loss\n",
        "        labels = input_ids.clone()\n",
        "        prompt_len = prompt[\"input_ids\"].shape[1]\n",
        "        labels[:prompt_len] = -100\n",
        "        labels[attention_mask == 0] = -100\n",
        "\n",
        "        return {\n",
        "            \"input_ids\": input_ids,\n",
        "            \"attention_mask\": attention_mask,\n",
        "            \"pixel_values\": pixel_values,\n",
        "            \"labels\": labels,\n",
        "        }\n"
      ],
      "metadata": {
        "id": "tAVf7DLB0Gwp"
      },
      "id": "tAVf7DLB0Gwp",
      "execution_count": 41,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import torch\n",
        "\n",
        "def gemma_mm_pad_collator(features):\n",
        "    # Max length based on input_ids\n",
        "    max_len = max(f[\"input_ids\"].shape[0] for f in features)\n",
        "\n",
        "    pad_id = getattr(processor.tokenizer, \"pad_token_id\", 0)\n",
        "    if pad_id is None:\n",
        "        pad_id = 0\n",
        "\n",
        "    def pad_1d(x, pad_value):\n",
        "        pad_len = max_len - x.shape[0]\n",
        "        if pad_len <= 0:\n",
        "            return x\n",
        "        return torch.cat([x, torch.full((pad_len,), pad_value, dtype=x.dtype, device=x.device)], dim=0)\n",
        "\n",
        "    def pad_2d(x, pad_value):\n",
        "        # pad on first dimension only (seq_len, hidden/whatever)\n",
        "        pad_len = max_len - x.shape[0]\n",
        "        if pad_len <= 0:\n",
        "            return x\n",
        "        pad_rows = torch.full((pad_len, x.shape[1]), pad_value, dtype=x.dtype, device=x.device)\n",
        "        return torch.cat([x, pad_rows], dim=0)\n",
        "\n",
        "    batch = {}\n",
        "    keys = features[0].keys()\n",
        "\n",
        "    for k in keys:\n",
        "        vals = [f[k] for f in features]\n",
        "\n",
        "        # Non-tensors: keep as list\n",
        "        if not torch.is_tensor(vals[0]):\n",
        "            batch[k] = vals\n",
        "            continue\n",
        "\n",
        "        # If scalar tensors: stack directly\n",
        "        if vals[0].ndim == 0:\n",
        "            batch[k] = torch.stack(vals)\n",
        "            continue\n",
        "\n",
        "        # If 1D tensors and lengths vary: pad to max_len\n",
        "        if vals[0].ndim == 1:\n",
        "            lens = [v.shape[0] for v in vals]\n",
        "            if len(set(lens)) > 1:\n",
        "                if k == \"input_ids\":\n",
        "                    pad_value = pad_id\n",
        "                elif k == \"attention_mask\":\n",
        "                    pad_value = 0\n",
        "                elif k == \"labels\":\n",
        "                    pad_value = -100\n",
        "                else:\n",
        "                    pad_value = 0  # safe default for extra seq fields (position_ids, etc.)\n",
        "                vals = [pad_1d(v, pad_value) for v in vals]\n",
        "            batch[k] = torch.stack(vals)\n",
        "            continue\n",
        "\n",
        "        # If 2D tensors and first dim varies: pad to max_len on dim0\n",
        "        if vals[0].ndim == 2:\n",
        "            lens = [v.shape[0] for v in vals]\n",
        "            if len(set(lens)) > 1:\n",
        "                # labels-like 2D is rare; 0 is usually safe\n",
        "                vals = [pad_2d(v, 0) for v in vals]\n",
        "            batch[k] = torch.stack(vals)\n",
        "            continue\n",
        "\n",
        "        # Higher dims (pixel_values etc.) should already be same shape\n",
        "        batch[k] = torch.stack(vals)\n",
        "\n",
        "    return batch\n"
      ],
      "metadata": {
        "id": "rt0xv8KE_RS0"
      },
      "id": "rt0xv8KE_RS0",
      "execution_count": 51,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import torch\n",
        "from torch.utils.data import DataLoader, WeightedRandomSampler, Dataset\n",
        "from transformers import TrainingArguments, Trainer, default_data_collator\n",
        "from collections import Counter\n",
        "\n",
        "# -------------------------\n",
        "# 1) Balanced sampler (train only)\n",
        "# -------------------------\n",
        "def make_balanced_sampler(labels):\n",
        "    counts = Counter([int(x) for x in labels])\n",
        "    w0 = 1.0 / max(counts.get(0, 1), 1)\n",
        "    w1 = 1.0 / max(counts.get(1, 1), 1)\n",
        "    weights = [w0 if int(y) == 0 else w1 for y in labels]\n",
        "    return WeightedRandomSampler(\n",
        "        torch.DoubleTensor(weights),\n",
        "        num_samples=len(weights),\n",
        "        replacement=True\n",
        "    )\n",
        "\n",
        "train_sampler = make_balanced_sampler(train_labels)\n",
        "print(\"\u2705 Sampler ready (balanced batches)\")\n",
        "\n",
        "# -------------------------\n",
        "# 2) Dataset (FIXED: proper answer-only loss masking)\n",
        "# -------------------------\n",
        "class LeukemiaAnswerOnlyDataset(Dataset):\n",
        "    def __init__(self, images, labels, processor):\n",
        "        self.images = images\n",
        "        self.labels = [int(x) for x in labels]\n",
        "        self.processor = processor\n",
        "        self.answe0rs = {0: \"Normal\", 1: \"Leukemia\"}\n",
        "\n",
        "        self.user_text = (\n",
        "            \"Classify this blood cell microscopy image.\\n\"\n",
        "            \"Answer with exactly ONE word: Normal or Leukemia.\\n\"\n",
        "            \"Answer:\"\n",
        "        )\n",
        "\n",
        "    def __len__(self):\n",
        "        return len(self.labels)\n",
        "\n",
        "    def __getitem__(self, idx):\n",
        "        image = self.images[idx]\n",
        "        y = self.labels[idx]\n",
        "        answer = self.answers[y]\n",
        "\n",
        "        # \u2705 Full message WITH answer\n",
        "        msgs_full = [\n",
        "            {\"role\": \"user\", \"content\": [{\"type\": \"image\"}, {\"type\": \"text\", \"text\": self.user_text}]},\n",
        "            {\"role\": \"assistant\", \"content\": [{\"type\": \"text\", \"text\": answer}]},\n",
        "        ]\n",
        "\n",
        "        # \u2705 FIX: Prompt only (NO assistant turn, let add_generation_prompt add it)\n",
        "        msgs_prompt = [\n",
        "            {\"role\": \"user\", \"content\": [{\"type\": \"image\"}, {\"type\": \"text\", \"text\": self.user_text}]},\n",
        "        ]\n",
        "\n",
        "        # \u2705 FIX: add_generation_prompt=True for prompt marks where answer starts\n",
        "        text_full = self.processor.apply_chat_template(msgs_full, add_generation_prompt=False)\n",
        "        text_prompt = self.processor.apply_chat_template(msgs_prompt, add_generation_prompt=True)  # \u2190 KEY FIX\n",
        "\n",
        "        full = self.processor(\n",
        "            images=image,\n",
        "            text=text_full,\n",
        "            return_tensors=\"pt\",\n",
        "            padding=False,\n",
        "            truncation=False,\n",
        "        )\n",
        "        prompt = self.processor(\n",
        "            images=image,\n",
        "            text=text_prompt,\n",
        "            return_tensors=\"pt\",\n",
        "            padding=False,\n",
        "            truncation=False,\n",
        "        )\n",
        "\n",
        "        # Squeeze batch dim\n",
        "        full = {k: v.squeeze(0) for k, v in full.items()}\n",
        "\n",
        "        # \u2705 FIX: Use shape[1] for prompt length (before squeeze)\n",
        "        prompt_len = prompt[\"input_ids\"].shape[1]\n",
        "\n",
        "        # Answer-only loss: mask everything up to prompt length\n",
        "        labels = full[\"input_ids\"].clone()\n",
        "        labels[:prompt_len] = -100\n",
        "        full[\"labels\"] = labels\n",
        "\n",
        "        return full\n",
        "\n",
        "train_dataset = LeukemiaAnswerOnlyDataset(train_images, train_labels, processor)\n",
        "val_dataset   = LeukemiaAnswerOnlyDataset(val_images, val_labels, processor)\n",
        "\n",
        "print(\"\u2705 Datasets ready\")\n",
        "print(\"Train:\", len(train_dataset), \" Val:\", len(val_dataset))\n",
        "\n",
        "# \u2705 Verify fix\n",
        "sample = train_dataset[0]\n",
        "trainable = sample[\"labels\"][sample[\"labels\"] != -100]\n",
        "print(f\"\u2705 Trainable tokens: {len(trainable)}\")\n",
        "print(f\"\u2705 Decoded: {processor.decode(trainable)}\")\n",
        "\n",
        "# -------------------------\n",
        "# 3) Custom Trainer to inject sampler\n",
        "# -------------------------\n",
        "class BalancedTrainer(Trainer):\n",
        "    def get_train_dataloader(self):\n",
        "        return DataLoader(\n",
        "            self.train_dataset,\n",
        "            batch_size=self.args.per_device_train_batch_size,\n",
        "            sampler=train_sampler,\n",
        "            collate_fn=gemma_mm_pad_collator,\n",
        "            num_workers=2,\n",
        "            pin_memory=True,\n",
        "        )\n",
        "\n",
        "# -------------------------\n",
        "# 4) TrainingArguments\n",
        "# -------------------------\n",
        "OUTPUT_DIR = \"/content/medgemma_lora_run_v2\"\n",
        "\n",
        "args = TrainingArguments(\n",
        "    output_dir=OUTPUT_DIR,\n",
        "    per_device_train_batch_size=4,\n",
        "    per_device_eval_batch_size=4,\n",
        "    gradient_accumulation_steps=8,\n",
        "\n",
        "    num_train_epochs=5,\n",
        "    learning_rate=2e-5,\n",
        "    warmup_ratio=0.05,\n",
        "    weight_decay=0.01,\n",
        "\n",
        "    bf16=True,\n",
        "\n",
        "    logging_steps=25,\n",
        "    eval_strategy=\"steps\",\n",
        "    eval_steps=150,\n",
        "    save_strategy=\"steps\",\n",
        "    save_steps=150,\n",
        "    save_total_limit=3,\n",
        "\n",
        "    load_best_model_at_end=True,\n",
        "    metric_for_best_model=\"eval_loss\",\n",
        "    greater_is_better=False,\n",
        "\n",
        "    remove_unused_columns=False,\n",
        "    report_to=\"none\",\n",
        ")\n",
        "\n",
        "# -------------------------\n",
        "# 5) Build trainer\n",
        "# -------------------------\n",
        "trainer = BalancedTrainer(\n",
        "    model=model,\n",
        "    args=args,\n",
        "    train_dataset=train_dataset,\n",
        "    eval_dataset=val_dataset,\n",
        "    data_collator=gemma_mm_pad_collator,\n",
        ")\n",
        "\n",
        "print(\"\u2705 Trainer ready - run trainer.train()\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "vnhrbLY40LAU",
        "outputId": "07ada47d-21d8-4da5-e0e2-a86d85f8de08"
      },
      "id": "vnhrbLY40LAU",
      "execution_count": 63,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "warmup_ratio is deprecated and will be removed in v5.2. Use `warmup_steps` instead.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\u2705 Sampler ready (balanced batches)\n",
            "\u2705 Datasets ready\n",
            "Train: 13951  Val: 1744\n",
            "\u2705 Trainable tokens: 5\n",
            "\u2705 Decoded: Leukemia<end_of_turn>\n",
            "\n",
            "\u2705 Trainer ready - run trainer.train()\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# -------------------------\n",
        "# 6) Train\n",
        "# -------------------------\n",
        "print(\"\\n\ud83d\ude80 Starting LoRA fine-tuning...\")\n",
        "print(\"Checkpoints will be saved to:\", OUTPUT_DIR)\n",
        "print(\"-\" * 60)\n",
        "\n",
        "trainer.train()\n",
        "\n",
        "print(\"\\n\u2705 Training complete!\")\n",
        "print(\"Best checkpoint:\", trainer.state.best_model_checkpoint)\n",
        "\n"
      ],
      "metadata": {
        "id": "Wk1kcsfY0a3j",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 639
        },
        "outputId": "73e98cfb-1cbb-4718-dcaa-2ab15374dfe6"
      },
      "id": "Wk1kcsfY0a3j",
      "execution_count": 64,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "\ud83d\ude80 Starting LoRA fine-tuning...\n",
            "Checkpoints will be saved to: /content/medgemma_lora_run_v2\n",
            "------------------------------------------------------------\n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ],
            "text/html": [
              "\n",
              "    <div>\n",
              "      \n",
              "      <progress value='2180' max='2180' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
              "      [2180/2180 5:33:08, Epoch 5/5]\n",
              "    </div>\n",
              "    <table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              " <tr style=\"text-align: left;\">\n",
              "      <th>Step</th>\n",
              "      <th>Training Loss</th>\n",
              "      <th>Validation Loss</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <td>150</td>\n",
              "      <td>0.136213</td>\n",
              "      <td>0.122787</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>300</td>\n",
              "      <td>0.114943</td>\n",
              "      <td>0.087362</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>450</td>\n",
              "      <td>0.063766</td>\n",
              "      <td>0.066102</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>600</td>\n",
              "      <td>0.055370</td>\n",
              "      <td>0.052734</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>750</td>\n",
              "      <td>0.050667</td>\n",
              "      <td>0.055197</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>900</td>\n",
              "      <td>0.043557</td>\n",
              "      <td>0.050898</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>1050</td>\n",
              "      <td>0.027168</td>\n",
              "      <td>0.043081</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>1200</td>\n",
              "      <td>0.045397</td>\n",
              "      <td>0.057755</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>1350</td>\n",
              "      <td>0.028797</td>\n",
              "      <td>0.034892</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>1500</td>\n",
              "      <td>0.022488</td>\n",
              "      <td>0.034721</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>1650</td>\n",
              "      <td>0.026137</td>\n",
              "      <td>0.038007</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>1800</td>\n",
              "      <td>0.020790</td>\n",
              "      <td>0.032950</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>1950</td>\n",
              "      <td>0.024392</td>\n",
              "      <td>0.036485</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>2100</td>\n",
              "      <td>0.014337</td>\n",
              "      <td>0.037194</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table><p>"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "\u2705 Training complete!\n",
            "Best checkpoint: /content/medgemma_lora_run_v2/checkpoint-1800\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# -------------------------\n",
        "# 7) Save final model + processor\n",
        "# -------------------------\n",
        "final_dir = \"/content/medgemma_lora_final\"\n",
        "trainer.save_model(final_dir)\n",
        "processor.save_pretrained(final_dir)\n",
        "\n",
        "print(\"\u2705 Saved final LoRA adapter + processor to:\", final_dir)"
      ],
      "metadata": {
        "id": "ZuqU4a3S08GZ",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "2fdb3d0e-9d19-4757-c8a2-c5a7351a92ff"
      },
      "id": "ZuqU4a3S08GZ",
      "execution_count": 65,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\u2705 Saved final LoRA adapter + processor to: /content/medgemma_lora_final\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# OPTIONAL: Push to Hugging Face Hub\n",
        "# Uncomment to upload your model\n",
        "\n",
        "HF_REPO = \"good2idnan/medgemma-1.5-4b-it-leukemia-lora\"\n",
        "model.push_to_hub(HF_REPO)\n",
        "processor.push_to_hub(HF_REPO)\n",
        "print(f\"\u2705 Pushed to: https://huggingface.co/good2idnan/medgemma-1.5-4b-it-leukemia-lora\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 259,
          "referenced_widgets": [
            "5e38a5dad6ef403cb9fafab6308ba672",
            "ada913033a014c29a1c109fa6afcdce0",
            "e304e4a2468245ddadf73da94d894206",
            "9019db7d43cc4b4d9230b12acb71b102",
            "c59d893b9ef444a2a04144784b3d1879",
            "d1b76640090f4dd6888dcb3c8b21cec9",
            "e1d22012a2cb4a899ffd250352c2d14b",
            "4d237786eb0447038743a7fe2717e4e8",
            "ef2df99914524a6ca67861b3c1dca30e",
            "7e9be2647042496ea55330a066dbb1c5",
            "7015acd2a12a4483974b03d81c7af07a",
            "79402685cc47487392a60fcd8acd1c15",
            "2b2ccf98af9a4fd78dab4ab76e4b98db",
            "7b14c37af66f4fc5a8d4cdd2de049887",
            "6063b4356489417e85dec8253c5398a5",
            "63ae6dd8d344440e8dc1ae1313cc20b4",
            "d1cf9c45befe45ff9681c36fad3561d1",
            "80281941c45147cd92e2c8c47bb014eb",
            "cd5ce124aa6843478c70e7e759e91cc6",
            "f4f1f587dbe043c6a333e6d47f395293",
            "23e0b50482e9480e8438a1020e91c9e5",
            "725570dd873543458861cbb99f9c1e11",
            "5ed311437a8048b2a1bee7e2f60e8b1d",
            "9f5b9989fb32496eac20f3bbcd86811f",
            "9a6ee24c3cd3446c8610015d2e1baf7e",
            "0863aa5f822440f5ae5c1c9a8a153ea2",
            "6b999c7da7dc430ab2d41d66d3b84959",
            "713e4e9b5aa14aa0934ef33d71c9c3af",
            "5e4964d3f8244baf89261f28382c1648",
            "768944dbf5d1446f8e69434dc2a9e2c6",
            "88fdadaeef00410eb64d3db2e72f840e",
            "34a035d040ee495193d939fc6b7a1677",
            "acff09bf48bb402baa4764d166aa1126",
            "eed93e606b1f4e2dbc06bd9bddfc4b41",
            "cd855c028aee4cadb6dd9d11c5a632fe",
            "f01e92cfec5247e48537959fbafd831c",
            "82f63e5289e4479aae2bf89e0d21def9",
            "36e589ba22c54bfe836df32290845fa7",
            "49d368e410744bebb32075cb5c540013",
            "799809ba51854ae0b6de5b0a489cc4a0",
            "50401bb73f52430f82f4406dea06748c",
            "f67f6837e61b44cc95947e317d91bb1d",
            "96aef22dda7543e6b53bcc8fbebc2eb1",
            "894da309fbe24761b24cdb65554ede0a",
            "b3598de2d96f44729d9233413c28f486",
            "84d0092e23464626a1e3ef20b471b1ca",
            "8f910a6e623e4ef4ba6040c9f9073a2d",
            "3b2e369659c04c74b0749599c9c6586f",
            "6f184b581cb74eee8756043727fe76f4",
            "9c8b55a01e2e47d7a1a5db9af5d744a4",
            "9d95a453abf94c03b3d8eed98b54cc79",
            "b5ff1ea9c9774dea8e885d897bc0c66a",
            "5e0e2944ff0047febf913a93f276def1",
            "b24247c3df954556a6f9a43de80b67b8",
            "7208b2ff4af34579935823f8b17450b3",
            "59dd1aa2213f49188d08b3fd47fc8604",
            "d099304fd671472ab3aba92e743bcaf1",
            "44d34c3fefc24dc888c08b95bfb3c874",
            "4c6a826c7bfa455eb2b7f9b60cdb68c7",
            "4e5e67f5968c499a9fa4af7b8ed7872d",
            "42b2bb7e57d945cca57fd11c57663953",
            "c82a1387a1444d1c873dfb718191344a",
            "13ca1066742445e8b0d2a1c035091da9",
            "1f307ea443924f2cb9899e23960ea355",
            "3c289fab095040e594b4fcb10bb64fc0",
            "d8aa6f20541c4414aff28f3836a3f444",
            "934b4fbf6240410c9446aab0ab9fbde4",
            "016de24504fb4d38b768a03036a09e7b",
            "a748576b8f344869a243e07ac27ec8c0",
            "d1723f2c381e4d0488ecac5486fb8966",
            "7a28174542714eb38c7258a1323c9239",
            "9de07fd0c6b24173bb3ce54ad32db9fb",
            "4838146f49b84b399d22559782774889",
            "e4e6067223b14f0ab11e56481c190962",
            "d5aafbddfa844ded858ea27dd4f183d5",
            "53eff652f27342899ea603410a725649",
            "25dfeb46a70c410f9d05a3f3e8742e8b"
          ]
        },
        "id": "SheiKOFFXe3N",
        "outputId": "ac694fb4-1800-4f37-e04b-1000692a2588"
      },
      "id": "SheiKOFFXe3N",
      "execution_count": 66,
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "Processing Files (0 / 0)      : |          |  0.00B /  0.00B            "
            ],
            "application/vnd.jupyter.widget-view+json": {
              "version_major": 2,
              "version_minor": 0,
              "model_id": "5e38a5dad6ef403cb9fafab6308ba672"
            }
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "New Data Upload               : |          |  0.00B /  0.00B            "
            ],
            "application/vnd.jupyter.widget-view+json": {
              "version_major": 2,
              "version_minor": 0,
              "model_id": "79402685cc47487392a60fcd8acd1c15"
            }
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "  ...adapter_model.safetensors:   1%|          |  563kB / 95.3MB            "
            ],
            "application/vnd.jupyter.widget-view+json": {
              "version_major": 2,
              "version_minor": 0,
              "model_id": "5ed311437a8048b2a1bee7e2f60e8b1d"
            }
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "README.md: 0.00B [00:00, ?B/s]"
            ],
            "application/vnd.jupyter.widget-view+json": {
              "version_major": 2,
              "version_minor": 0,
              "model_id": "eed93e606b1f4e2dbc06bd9bddfc4b41"
            }
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "Processing Files (0 / 0)      : |          |  0.00B /  0.00B            "
            ],
            "application/vnd.jupyter.widget-view+json": {
              "version_major": 2,
              "version_minor": 0,
              "model_id": "b3598de2d96f44729d9233413c28f486"
            }
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "New Data Upload               : |          |  0.00B /  0.00B            "
            ],
            "application/vnd.jupyter.widget-view+json": {
              "version_major": 2,
              "version_minor": 0,
              "model_id": "59dd1aa2213f49188d08b3fd47fc8604"
            }
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "  ...mpsjz33r2e/tokenizer.json:  23%|##2       | 7.61MB / 33.4MB            "
            ],
            "application/vnd.jupyter.widget-view+json": {
              "version_major": 2,
              "version_minor": 0,
              "model_id": "934b4fbf6240410c9446aab0ab9fbde4"
            }
          },
          "metadata": {}
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\u2705 Pushed to: https://huggingface.co/good2idnan/medgemma-1.5-4b-it-leukemia-lora\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from sklearn.metrics import classification_report, f1_score, confusion_matrix\n",
        "import torch\n",
        "\n",
        "model.eval()\n",
        "predictions, truths = [], []\n",
        "\n",
        "# Evaluate on validation set\n",
        "for i in range(len(val_images)):\n",
        "    msgs = [{\"role\": \"user\", \"content\": [\n",
        "        {\"type\": \"image\"},\n",
        "        {\"type\": \"text\", \"text\": \"Classify this blood cell microscopy image.\\nAnswer with exactly ONE word: Normal or Leukemia.\\nAnswer:\"}\n",
        "    ]}]\n",
        "    text = processor.apply_chat_template(msgs, add_generation_prompt=True)\n",
        "    inputs = processor(images=val_images[i], text=text, return_tensors=\"pt\").to(model.device)\n",
        "\n",
        "    with torch.no_grad():\n",
        "        output = model.generate(**inputs, max_new_tokens=5, do_sample=False)\n",
        "\n",
        "    response = processor.decode(output[0], skip_special_tokens=True).lower()\n",
        "    pred = 1 if \"leukemia\" in response else 0\n",
        "\n",
        "    predictions.append(pred)\n",
        "    truths.append(int(val_labels[i]))\n",
        "\n",
        "    if (i + 1) % 200 == 0:\n",
        "        print(f\"Evaluated {i+1}/{len(val_images)}\")\n",
        "\n",
        "# Print results\n",
        "print(\"\\n\" + \"=\"*60)\n",
        "print(\"FINAL EVALUATION RESULTS\")\n",
        "print(\"=\"*60)\n",
        "print(classification_report(truths, predictions, target_names=[\"Normal\", \"Leukemia\"]))\n",
        "print(f\"\ud83c\udfaf Weighted F1 Score: {f1_score(truths, predictions, average='weighted'):.4f}\")\n",
        "print(f\"\\nConfusion Matrix:\\n{confusion_matrix(truths, predictions)}\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 216
        },
        "id": "-GwX7Q2lj2FU",
        "outputId": "403dba16-50c7-443e-8e5d-18f8a39859b1"
      },
      "id": "-GwX7Q2lj2FU",
      "execution_count": 2,
      "outputs": [
        {
          "output_type": "error",
          "ename": "NameError",
          "evalue": "name 'model' is not defined",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
            "\u001b[0;32m/tmp/ipython-input-705940432.py\u001b[0m in \u001b[0;36m<cell line: 0>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      2\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mtorch\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      3\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 4\u001b[0;31m \u001b[0mmodel\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0meval\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      5\u001b[0m \u001b[0mpredictions\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtruths\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      6\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mNameError\u001b[0m: name 'model' is not defined"
          ]
        }
      ]
    }
  ],
  "metadata": {
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python",
      "version": "3"
    },
    "colab": {
      "provenance": [],
      "gpuType": "A100"
    },
    "accelerator": "GPU"
  },
  "nbformat": 4,
  "nbformat_minor": 5
}